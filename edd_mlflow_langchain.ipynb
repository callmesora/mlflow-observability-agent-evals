{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1921ef3",
   "metadata": {},
   "source": [
    "# Evaluation-Driven Development (EDD) with MLflow & LangChain\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "In this hands-on workshop, we'll master **Evaluation-Driven Development (EDD)** for GenAI applicationsâ€”a systematic approach to building reliable AI systems through continuous, automated evaluation.\n",
    "\n",
    "### Learning Objectives\n",
    "- **Understand EDD Philosophy**: Why rigorous evaluation should drive development decisions\n",
    "- **Build RAG Evaluation Pipelines**: Evaluate retrieval quality, relevance, and correctness\n",
    "- **Evaluate Agentic Systems**: Measure tool usage efficiency and task completion\n",
    "- **Design Effective Test Cases**: Create realistic scenarios that catch edge cases\n",
    "- **Instrument Traces for Insights**: Use MLflow to capture and analyze agent behavior\n",
    "\n",
    "### What We'll Build Today\n",
    "\n",
    "**Scenario 1: RAG System Evaluation**\n",
    "- A product support bot that retrieves relevant device specifications\n",
    "- Evaluates retrieval quality (did it find the RIGHT documents?)\n",
    "- Evaluates answer correctness (did it synthesize the right answer?)\n",
    "- Tests multi-hop reasoning (questions requiring multiple document types)\n",
    "\n",
    "**Scenario 2: Agent System Evaluation**\n",
    "- A CMS manager agent that searches, retrieves details, and publishes articles\n",
    "- Evaluates task completion (did it accomplish the goal?)\n",
    "- Analyzes tool trajectories (did it use tools efficiently and follow SOP?)\n",
    "- Validates decision-making (does it avoid redundant operations?)\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "âœ“ **Environment Setup:**\n",
    "- Python 3.10+ installed\n",
    "- `OPENAI_API_KEY` set in environment or `.env` file\n",
    "- All dependencies installed (see Import Setup below)\n",
    "\n",
    "âœ“ **Recommended Background:**\n",
    "- Familiarity with LangChain or LLM chains\n",
    "- Basic understanding of RAG concepts\n",
    "- Comfort with Python and Jupyter notebooks\n",
    "\n",
    "### How to Use This Notebook\n",
    "\n",
    "This notebook is designed as a **follow-along** conference presentation:\n",
    "1. Read the **Context & Explanation** sections to understand concepts\n",
    "2. Run **Code cells** in sequence (don't skip!)\n",
    "3. Check **Outputs** to verify each step is working\n",
    "4. At key points, **pause and experiment**â€”modify prompts, test cases, or parameters\n",
    "5. Refer to the **Next Steps** section at the end for real-world application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619f3640",
   "metadata": {},
   "source": [
    "# Part 0: Setup & Environment Configuration\n",
    "\n",
    "## Install & Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "364e9d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !uv sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07a7f048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies if running in Colab or a fresh environment\n",
    "#%pip install -q \"mlflow>=2.14\" langgraph langchain langchain-openai langchain-community langchain-text-splitters faiss-cpu pandas openai python-dotenv bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbd8d61e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow version: 3.6.0\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "import os\n",
    "from langchain.tools import tool\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "from mlflow.genai.scorers import (\n",
    "    Correctness,\n",
    "    RelevanceToQuery,\n",
    "    Guidelines,\n",
    ")\n",
    "from mlflow.entities import Feedback, SpanType, Trace\n",
    "from mlflow.genai import scorer\n",
    "from deepeval.metrics import TaskCompletionMetric\n",
    "from deepeval.test_case import LLMTestCase, ToolCall\n",
    "import json\n",
    "from deepeval.metrics import ContextualRelevancyMetric\n",
    "\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "print(f\"MLflow version: {mlflow.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0f973e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pedro.azevedo/dspt-mlflow/.venv/lib/python3.13/site-packages/mlflow/tracking/_tracking_service/utils.py:140: FutureWarning: Filesystem tracking backend (e.g., './mlruns') is deprecated. Please switch to a database backend (e.g., 'sqlite:///mlflow.db'). For feedback, see: https://github.com/mlflow/mlflow/issues/18534\n",
      "  return FileStore(store_uri, store_uri)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///Users/pedro.azevedo/dspt-mlflow/mlruns/413835162552422093', creation_time=1763905836235, experiment_id='413835162552422093', last_update_time=1763905836235, lifecycle_stage='active', name='GenAI_Eval_Demo', tags={}>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load API Key\n",
    "load_dotenv()\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = input(\"Enter your OpenAI API Key: \")\n",
    "\n",
    "# Set a specific experiment for this notebook\n",
    "mlflow.set_experiment(\"GenAI_Eval_Demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26587c6",
   "metadata": {},
   "source": [
    "## Part 1: RAG Evaluation\n",
    "\n",
    "We will build a simple RAG chain that answers questions about software tools. We will then evaluate it using MLflow's **\"Trace Required\"** judges, which inspect the actual retrieved documents to ensure relevance and groundedness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac6fc73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Enable Autologging\n",
    "mlflow.langchain.autolog()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef18f079",
   "metadata": {},
   "source": [
    "## Understanding MLflow Traces: Foundation for Evaluation\n",
    "\n",
    "When an agent runs, MLflow captures the **entire execution journey** in a hierarchical structure called a **Trace**:\n",
    "\n",
    "```\n",
    "Trace (entire execution)\n",
    "â”œâ”€â”€ Span: LLM Call (to ChatGPT)\n",
    "â”œâ”€â”€ Span: Tool Call (search_articles)\n",
    "â”‚   â”œâ”€â”€ Input: {\"query\": \"summer\"}\n",
    "â”‚   â”œâ”€â”€ Output: [{\"id\": \"102\", \"title\": \"Summer Recipes\"}]\n",
    "â”‚   â””â”€â”€ Duration: 234ms\n",
    "â”œâ”€â”€ Span: Tool Call (get_article_details)\n",
    "â”‚   â”œâ”€â”€ Input: {\"article_id\": \"102\"}\n",
    "â”‚   â”œâ”€â”€ Output: {\"title\": \"...\", \"status\": \"published\"}\n",
    "â”‚   â””â”€â”€ Duration: 156ms\n",
    "â””â”€â”€ Span: LLM Call (synthesis)\n",
    "```\n",
    "\n",
    "**Why Extract Traces?** \n",
    "When we evaluate agents, we need to verify not just the final answer, but also:\n",
    "- Which documents were retrieved (for RAG correctness)\n",
    "- Which tools were called and in what order (for tool trajectory analysis)\n",
    "- What decisions the agent made based on tool outputs\n",
    "\n",
    "The functions below parse these nested trace structures to extract the specific data needed for evaluation.\n",
    "\n",
    "### Helper Functions: Parsing Trace Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a104a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def extract_source_nodes(json_input):\n",
    "    \"\"\"\n",
    "    Parses a JSON string containing a message history and extracts source nodes\n",
    "    from tool artifacts.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        parsed_data = json.loads(json_input)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error parsing JSON: {e}\")\n",
    "        return []\n",
    "\n",
    "    # Handle the structure: {\"messages\": [...]}\n",
    "    messages = parsed_data.get(\"messages\", []) if isinstance(parsed_data, dict) else []\n",
    "    \n",
    "    source_nodes = []\n",
    "    \n",
    "    for message in messages:\n",
    "        # We are looking for messages where type is 'tool' and an 'artifact' list exists\n",
    "        if message.get(\"type\") == \"tool\" and \"artifact\" in message:\n",
    "            artifacts = message[\"artifact\"]\n",
    "            \n",
    "            # Ensure artifact is a list before extending our results\n",
    "            if isinstance(artifacts, list):\n",
    "                source_nodes.extend(artifacts)\n",
    "                \n",
    "    return source_nodes\n",
    "\n",
    "def extract_final_response(json_input):\n",
    "    \"\"\"\n",
    "    Parses a JSON string and extracts the content of the final AI response.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        parsed_data = json.loads(json_input)\n",
    "    except json.JSONDecodeError:\n",
    "        return None\n",
    "\n",
    "    messages = parsed_data.get(\"messages\", []) if isinstance(parsed_data, dict) else []\n",
    "    \n",
    "    # Iterate backwards to find the most recent AI message with content\n",
    "    for message in reversed(messages):\n",
    "        if message.get(\"type\") == \"ai\" and message.get(\"content\"):\n",
    "            return message[\"content\"]\n",
    "            \n",
    "    return None\n",
    "\n",
    "def _extract_deepeval_components(trace : Trace):\n",
    "    \"\"\"Extract input, output, and context from trace data\"\"\"\n",
    "    request = str(trace.data.request)\n",
    "    response = str(trace.data.response)\n",
    "\n",
    "    # extract source nodes if they exist\n",
    "    # Extract Source Nodes\n",
    "    outputs = extract_source_nodes(response)\n",
    "    retrieval_context = [node['page_content'] for node in outputs]\n",
    "    \n",
    "    \n",
    "    actual_output = extract_final_response(response)\n",
    "\n",
    "    return {\n",
    "        'input': request,\n",
    "        'actual_output': actual_output,\n",
    "        'retrieval_context': retrieval_context\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199af70d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: RAG Evaluation â€“ Building a Product Support Bot\n",
    "\n",
    "## 1.1 Scenario: Evaluating Retrieval-Augmented Generation\n",
    "\n",
    "**Problem:** We have a knowledge base of product specifications. We want to verify that when users ask questions, the bot:\n",
    "1. âœ“ Retrieves the **correct** documents\n",
    "2. âœ“ Uses retrieved information to **answer correctly**\n",
    "3. âœ“ Handles **multi-hop** questions (requiring info from multiple documents)\n",
    "\n",
    "**Example Multi-Hop Question:**\n",
    "- User: *\"Can I use the Orbit Watch Pro with the Orbit Phone X10?\"*\n",
    "- Requires retrieving: Phone specs (OS version) + Watch specs (OS requirements)\n",
    "- Bad retrieval: Only returns Watch specs â†’ Can't verify compatibility\n",
    "- Good retrieval: Returns both â†’ Can verify match\n",
    "\n",
    "**Our Approach:**\n",
    "- Build a simple retriever with 5 product documents\n",
    "- Create test cases that require multi-hop reasoning\n",
    "- Define scorers that check both retrieval quality AND answer correctness\n",
    "- Use MLflow to track and compare evaluation runs\n",
    "\n",
    "---\n",
    "\n",
    "## 1.2 Creating the Knowledge Base\n",
    "\n",
    "We'll define product specifications as exact string variables. **This is crucial for evaluation**: \n",
    "when we later verify what was retrieved, we'll compare against these exact strings to ensure perfect traceability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e523bf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define content as variables to ensure 100% match between VectorStore and Eval Dataset\n",
    "rag_content_phone = \"Orbit Phone X10 Specs: Runs OrbitOS 4.0, uses USB-C charging port, supports 5G, release date Jan 2024.\"\n",
    "rag_content_watch = \"Orbit Watch Pro Specs: Requires phone running OrbitOS 4.0 or higher to sync. Battery life 24h.\"\n",
    "rag_content_buds = \"Orbit Buds Lite Specs: Connects via Bluetooth 5.0. Compatible with any device supporting Bluetooth.\"\n",
    "rag_content_old_charger = \"Legacy Charger Adapter: This adapter converts Micro-USB to USB-C. Max output 5W.\"\n",
    "rag_content_new_charger = \"Orbit FastCharger: Native USB-C charger. Output 30W. Required for fast charging on X10.\"\n",
    "\n",
    "rag_docs = [\n",
    "    Document(page_content=rag_content_phone, metadata={\"id\": \"doc_1\"}),\n",
    "    Document(page_content=rag_content_watch, metadata={\"id\": \"doc_2\"}),\n",
    "    Document(page_content=rag_content_buds, metadata={\"id\": \"doc_3\"}),\n",
    "    Document(page_content=rag_content_old_charger, metadata={\"id\": \"doc_4\"}),\n",
    "    Document(page_content=rag_content_new_charger, metadata={\"id\": \"doc_5\"}),\n",
    "]\n",
    "\n",
    "# Create Vector Store & Retriever\n",
    "vectorstore = FAISS.from_documents(rag_docs, OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever(k=1)\n",
    "\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve_context(query: str):\n",
    "    \"\"\"Retrieve information to help answer a query.\"\"\"\n",
    "    retrieved_docs = vectorstore.similarity_search(query, k=2)\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        (f\"Source: {doc.metadata}\\nContent: {doc.page_content}\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    return serialized, retrieved_docs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6487fd72",
   "metadata": {},
   "source": [
    "## 1.3 Building the RAG Agent\n",
    "\n",
    "### Key Design: response_format=\"content_and_artifact\"\n",
    "\n",
    "LangChain tools can return data in two formats:\n",
    "- **content**: Human-readable text for the agent's reasoning\n",
    "- **artifact**: Structured data (like Document objects) for downstream processing\n",
    "\n",
    "By using `response_format=\"content_and_artifact\"`, we tell MLflow to:\n",
    "1. Log the readable content for the agent\n",
    "2. **Also capture the raw artifacts** (the actual retrieved Documents) separately\n",
    "\n",
    "This is essential for evaluationâ€”we'll later verify not just that the agent answered well, but that it used the RIGHT documents.\n",
    "\n",
    "### The Tool: Retrieve Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86dbdb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_system_prompt = (\n",
    "    \"You are the Orbit Electronics Support Bot. \"\n",
    "    \"For every user question, you must retrieve specifications for ALL devices mentioned. \"\n",
    "    \"Synthesize the answer based ONLY on the retrieved text.\"\n",
    ")\n",
    "\n",
    "model = init_chat_model(\"gpt-4.1\")\n",
    "tools = [retrieve_context]\n",
    "\n",
    "\n",
    "rag_agent = create_agent(model, tools, system_prompt=rag_system_prompt)\n",
    "\n",
    "def qa_predict_rag_fn(query: str) -> str:\n",
    "    response = rag_agent.invoke({\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": query}],\n",
    "    })\n",
    "    answer = response['messages'][-1].content\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e395618",
   "metadata": {},
   "source": [
    "### Wrapping the Agent: qa_predict_rag_fn\n",
    "\n",
    "Our prediction function wraps the agent with:\n",
    "- Consistent input formatting (user message)\n",
    "- Extraction of final response (from the last AI message)\n",
    "- Logging to MLflow (autolog captures traces automatically)\n",
    "\n",
    "This is the function we'll pass to `mlflow.genai.evaluate()` later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83999acf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Orbit Phone X10 uses a USB-C charging port. For fast charging, you need the Orbit FastCharger, which is a native USB-C charger with a 30W output.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_predict_rag_fn(\"What charger do I need for the Orbit Phone X10?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10197ca9",
   "metadata": {},
   "source": [
    "### Testing the Agent: Quick Sanity Check\n",
    "\n",
    "Let's verify the agent works before we run the full evaluation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be1d158",
   "metadata": {},
   "source": [
    "## 1.4 Designing Test Cases: The RAG Evaluation Dataset\n",
    "\n",
    "### Why Multi-Hop Test Cases Matter\n",
    "\n",
    "**Single-hop questions** (e.g., *\"What's the battery life of the Orbit Watch Pro?\"*):\n",
    "- Easy for RAG systems: retrieve one document, extract answer\n",
    "- Limited signal: doesn't test retrieval quality or reasoning\n",
    "\n",
    "**Multi-hop questions** (e.g., *\"Can I use the Orbit Watch Pro with the Orbit Phone X10?\"*):\n",
    "- Requires retrieving MULTIPLE document types\n",
    "- Tests both retrieval quality AND reasoning\n",
    "- Better indicator of real-world performance\n",
    "\n",
    "### Test Case Design Strategy\n",
    "\n",
    "For each test case, we specify:\n",
    "- **Input**: User query\n",
    "- **Expected Response**: What the answer should contain\n",
    "- **Expected Facts**: Which documents MUST be retrieved (verbatim strings)\n",
    "- **Retrieval Context**: The actual context to verify against\n",
    "\n",
    "Notice: We use the **exact same strings** (`rag_content_phone`, `rag_content_watch`) that we put in the vector store. This ensures perfect traceabilityâ€”we can verify the agent retrieved the right documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b954f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_eval_dataset = [\n",
    "    # Case 1: Multi-hop Compatibility\n",
    "    # Logic: User asks about Watch + Phone. \n",
    "    # Requirement: Must retrieve Phone Specs (OS version) AND Watch Specs (OS requirement).\n",
    "    {\n",
    "        \"inputs\": {\"query\": \"Can I use the Orbit Watch Pro with the Orbit Phone X10?\"},\n",
    "        \"expectations\": {\n",
    "            \"expected_response\": \"Yes. The Orbit Phone X10 runs OrbitOS 4.0, which matches the Orbit Watch Pro's requirement.\",\n",
    "            # VERBATIM MATCHES:\n",
    "            \"expected_facts\": [rag_content_phone, rag_content_watch],\n",
    "            \"retrieval_context\": [rag_content_phone, rag_content_watch] \n",
    "        }\n",
    "    },\n",
    "    # Case 2: Multi-hop Power/Charging\n",
    "    # Logic: User has Old Charger + New Phone. \n",
    "    # Requirement: Must retrieve Old Charger Specs (5W) AND New Charger Specs (Requirement for Fast Charge).\n",
    "    {\n",
    "        \"inputs\": {\"query\": \"Will the Legacy Charger Adapter allow me to fast charge my Orbit Phone X10?\"},\n",
    "        \"expectations\": {\n",
    "            \"expected_response\": \"No. The Legacy Adapter output is 5W, but the X10 requires 30W (Orbit FastCharger) to fast charge.\",\n",
    "            # VERBATIM MATCHES:\n",
    "            \"expected_facts\": [rag_content_old_charger, rag_content_new_charger],\n",
    "            \"retrieval_context\": [rag_content_old_charger, rag_content_new_charger]\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fae39fb",
   "metadata": {},
   "source": [
    "## 1.5 Configuring Evaluation Metrics: Scorers & Judges\n",
    "\n",
    "### Understanding Our Three Scorers\n",
    "\n",
    "| Scorer | What It Measures | When to Use |\n",
    "|--------|------------------|-------------|\n",
    "| **Correctness** | Is the answer factually correct? | Alwaysâ€”baseline quality metric |\n",
    "| **RelevanceToQuery** | Does the answer address the user's question? | Alwaysâ€”catches off-topic answers |\n",
    "| **ContextualRelevancy** | Are the retrieved documents actually relevant? | RAG-specificâ€”catches retrieval failures |\n",
    "\n",
    "### Scorer 1 & 2: Built-in MLflow Scorers\n",
    "\n",
    "`Correctness` and `RelevanceToQuery` are **built into MLflow**â€”they use LLM-as-a-judge to evaluate outputs without requiring custom logic.\n",
    "\n",
    "### Scorer 3: Custom Scorer Using DeepEval\n",
    "\n",
    "We're implementing a custom scorer called `contextual_relevancy` that uses **DeepEval's ContextualRelevancyMetric**. \n",
    "\n",
    "**Why DeepEval?** It provides domain-specific metrics for LLM evaluation that aren't in MLflow's standard library, giving us more precise control over what we measure.\n",
    "\n",
    "**How It Works:**\n",
    "1. Extract retrieved context from the trace\n",
    "2. Pass context + response to DeepEval\n",
    "3. DeepEval judges whether retrieved docs are relevant to the question\n",
    "4. Return a score (0.0-1.0) and reasoning to MLflow\n",
    "\n",
    "### Defining the Scorers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e23e3b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@scorer(name=\"Context Relevancy\")\n",
    "def contextual_relevancy(trace: Trace):\n",
    "    \"\"\"Evaluate RAG traces using DeepEval metrics\"\"\"\n",
    "    \n",
    "    data = _extract_deepeval_components(trace)\n",
    "\n",
    "    # Define DeepEval metrics   \n",
    "    \n",
    "\n",
    "    # Create test cases\n",
    "    test_case= LLMTestCase(\n",
    "            input=data['input'],\n",
    "            actual_output=data['actual_output'],\n",
    "            retrieval_context=data['retrieval_context']\n",
    "        )\n",
    "    \n",
    "    relevancy_metric = ContextualRelevancyMetric(threshold=0.7, model=\"gpt-4o-mini\",\n",
    "                                        include_reason=True)        \n",
    "        \n",
    "    # Run the metric evaluation\n",
    "    relevancy_metric.measure(test_case)\n",
    "\n",
    "    # Extract results\n",
    "    score = relevancy_metric.score\n",
    "    reason = relevancy_metric.reason\n",
    "\n",
    "    return Feedback(\n",
    "        value=score,\n",
    "        rationale=reason,\n",
    "    )\n",
    "\n",
    "# Business-specific guidelines\n",
    "business_guidelines = Guidelines(\n",
    "    name=\"toxicity_clear\",\n",
    "    guidelines=\"\"\"\n",
    "    The response should avoid toxic language and adhere to community guidelines.\n",
    "    \"\"\",\n",
    ")\n",
    "\n",
    "# Configure RAG-specific scorers\n",
    "rag_scorers = [\n",
    "    Correctness(\n",
    "       # model=\"litellm_proxy:/amazon.nova-micro-v1:0\",\n",
    "    ),\n",
    "    RelevanceToQuery(\n",
    "        name=\"AnswerRelevance\"\n",
    "        #model=\"litellm_proxy:/amazon.nova-micro-v1:0\",\n",
    "    ),\n",
    "    contextual_relevancy,\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67389f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/24 21:14:12 INFO mlflow.models.evaluation.utils.trace: Auto tracing is temporarily enabled during the model evaluation for computing some metrics and debugging. To disable tracing, call `mlflow.autolog(disable=True)`.\n",
      "/Users/pedro.azevedo/dspt-mlflow/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025/11/24 21:14:13 INFO mlflow.genai.utils.data_validation: Testing model prediction with the first sample in the dataset. To disable this check, set the MLFLOW_GENAI_EVAL_SKIP_TRACE_VALIDATION environment variable to True.\n",
      "2025/11/24 21:14:13 WARNING mlflow.tracing.fluent: Failed to start span LangGraph: 'NonRecordingSpan' object has no attribute 'context'. For full traceback, set logging level to debug.\n",
      "Evaluating:   0%|          | 0/2 [Elapsed: 00:00, Remaining: ?] "
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/pedro.azevedo/dspt-mlflow/.venv/lib/python3.13/site-packages/rich/live.py:256: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/pedro.azevedo/dspt-mlflow/.venv/lib/python3.13/site-packages/rich/live.py:256: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/pedro.azevedo/dspt-mlflow/.venv/lib/python3.13/site-packages/rich/live.py:256: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/pedro.azevedo/dspt-mlflow/.venv/lib/python3.13/site-packages/rich/live.py:256: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/pedro.azevedo/dspt-mlflow/.venv/lib/python3.13/site-packages/rich/live.py:256: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/pedro.azevedo/dspt-mlflow/.venv/lib/python3.13/site-packages/rich/live.py:256: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/pedro.azevedo/dspt-mlflow/.venv/lib/python3.13/site-packages/rich/live.py:256: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/pedro.azevedo/dspt-mlflow/.venv/lib/python3.13/site-packages/rich/live.py:256: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/pedro.azevedo/dspt-mlflow/.venv/lib/python3.13/site-packages/rich/live.py:256: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/pedro.azevedo/dspt-mlflow/.venv/lib/python3.13/site-packages/rich/live.py:256: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [Elapsed: 00:07, Remaining: 00:07] "
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [Elapsed: 00:09, Remaining: 00:00] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ¨ Evaluation completed.\n",
      "\n",
      "Metrics and evaluation results are logged to the MLflow run:\n",
      "  Run name: \u001b[94mSimple Langgraph Agent\u001b[0m\n",
      "  Run ID: \u001b[94m0cb62d5b82ef485cb37048232864abc9\u001b[0m\n",
      "\n",
      "To view the detailed evaluation results with sample-wise scores,\n",
      "open the \u001b[93m\u001b[1mTraces\u001b[0m tab in the Run page in the MLflow UI.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "with mlflow.start_run(run_name=\"Simple Langgraph Agent\"):\n",
    "\n",
    "    eval_results = mlflow.genai.evaluate(\n",
    "        data=rag_eval_dataset,\n",
    "        predict_fn=qa_predict_rag_fn,\n",
    "        scorers=rag_scorers,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bac5ec7",
   "metadata": {},
   "source": [
    "## 1.6 Running the RAG Evaluation\n",
    "\n",
    "Now we'll run evaluation across our test cases. MLflow will:\n",
    "1. Call `qa_predict_rag_fn()` for each test case\n",
    "2. Capture the full trace (retrieval, reasoning, response)\n",
    "3. Run each scorer against the trace\n",
    "4. Aggregate results and track in MLflow UI\n",
    "\n",
    "**What to Look For:**\n",
    "- Do all scorers report > 0.7? (Good baseline)\n",
    "- Do scores align? (If correctness is high but relevancy is low, something's wrong)\n",
    "- Check MLflow UI for detailed per-test-case results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bde0a6",
   "metadata": {},
   "source": [
    "### Viewing RAG Evaluation Results\n",
    "\n",
    "Below we display the aggregated metrics. You can also explore:\n",
    "- **MLflow UI**: `mlflow ui` in terminal â†’ Navigate to \"GenAI_Eval_Demo\" experiment\n",
    "- **Per-test breakdown**: Drill into each test case to see which failed and why\n",
    "- **Scorer reasoning**: Read the LLM judge's explanation for each score\n",
    "\n",
    "ðŸ’¡ **Tip**: If scores are low, check:\n",
    "1. Is the vector store returning relevant documents?\n",
    "2. Is the agent system prompt clear?\n",
    "3. Are test cases realistic?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba340fea",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Agent Evaluation â€“ Building a CMS Management Agent\n",
    "\n",
    "## 2.1 Scenario: Evaluating Agentic Tool Usage\n",
    "\n",
    "**Problem:** We have an agent that manages CMS articles. We want to verify it:\n",
    "1. âœ“ Uses tools **efficiently** (no redundant calls)\n",
    "2. âœ“ Follows **standard operating procedures** (SOP)\n",
    "3. âœ“ Makes **logical decisions** based on tool outputs\n",
    "4. âœ“ **Completes tasks** correctly\n",
    "\n",
    "**Example Scenario:**\n",
    "- User: *\"Find the Summer Recipes article and ensure it is published.\"*\n",
    "- Smart agent: \n",
    "  1. Search for article (get ID)\n",
    "  2. Get details (check current status)\n",
    "  3. Skip publish (because it's already published) â† Key insight!\n",
    "- Naive agent: Searches â†’ Publishes anyway (redundant operation)\n",
    "\n",
    "**Why This Matters:** \n",
    "In production, tool calls cost money (API fees) and time (latency). An agent that avoids unnecessary operations is more efficient. EDD helps us catch and fix these inefficiencies.\n",
    "\n",
    "---\n",
    "\n",
    "## 2.2 Setting Up the CMS Agent\n",
    "\n",
    "### Our Mock Database\n",
    "\n",
    "We'll use a simple in-memory database simulating article metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "381c4d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mock Database\n",
    "cms_db = {\n",
    "    \"101\": {\"title\": \"AI Trends 2024\", \"status\": \"draft\", \"tags\": [\"tech\"]},\n",
    "    \"102\": {\"title\": \"Summer Recipes\", \"status\": \"published\", \"tags\": [\"food\"]},\n",
    "}\n",
    "\n",
    "# We define expected output strings for our test cases to verify against\n",
    "EXPECTED_SEARCH_OUTPUT_102 = str([{\"id\": \"102\", \"title\": \"Summer Recipes\", \"status\": \"published\"}])\n",
    "EXPECTED_DETAILS_OUTPUT_102 = str({\"title\": \"Summer Recipes\", \"status\": \"published\", \"tags\": [\"food\"]})\n",
    "\n",
    "\n",
    "# Tools \n",
    "@tool\n",
    "def search_articles(query: str):\n",
    "    \"\"\"Searches for articles by title. Returns JSON string of matches.\"\"\"\n",
    "    # Simple logic to mimic a search engine\n",
    "    results = [{\"id\": k, \"title\": v[\"title\"], \"status\": v[\"status\"]} \n",
    "               for k, v in cms_db.items() if query.lower() in v[\"title\"].lower()]\n",
    "    return str(results)\n",
    "\n",
    "@tool\n",
    "def get_article_details(article_id: str):\n",
    "    \"\"\"Gets full details for an ID.\"\"\"\n",
    "    return str(cms_db.get(article_id, \"Article not found\"))\n",
    "\n",
    "@tool\n",
    "def publish_article(article_id: str):\n",
    "    \"\"\"Publishes an article.\"\"\"\n",
    "    if article_id in cms_db:\n",
    "        cms_db[article_id][\"status\"] = \"published\"\n",
    "        return f\"Success: Article {article_id} published.\"\n",
    "    return \"Error: ID not found.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d2b53dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Tools + LLM\n",
    "cms_tools = [search_articles, get_article_details, publish_article]\n",
    "chat_model = init_chat_model(\"gpt-4.1\")\n",
    "\n",
    "# Setup Prompt\n",
    "agent_system_prompt = (\n",
    "    \"You are a CMS Manager. \"\n",
    "    \"SOP: Always SEARCH for an article ID first. Never guess IDs. \"\n",
    "    \"Before publishing, GET DETAILS to confirm the current status.\"\n",
    ")\n",
    "\n",
    "# Setup Agent\n",
    "cms_agent = create_agent(model=chat_model, tools=cms_tools, system_prompt=agent_system_prompt)\n",
    "\n",
    "# Prediction Function\n",
    "def agent_predict_fn(query: str) -> str:\n",
    "    response = cms_agent.invoke({\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": query}],\n",
    "    })\n",
    "    answer = response['messages'][-1].content\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31efd6ee",
   "metadata": {},
   "source": [
    "### Agent Configuration: Tools & System Prompt\n",
    "\n",
    "Our CMS manager has three tools:\n",
    "1. **search_articles**: Find articles by title\n",
    "2. **get_article_details**: Retrieve full metadata for an article\n",
    "3. **publish_article**: Change status to \"published\"\n",
    "\n",
    "Notice the system prompt: **\"Always SEARCH for an article ID first. Never guess IDs.\"**\n",
    "\n",
    "This establishes a **Standard Operating Procedure (SOP)** that we can later verify the agent follows. Good agents internalize this guidance; bad agents ignore it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd5ba0e",
   "metadata": {},
   "source": [
    "## 2.3 Designing Agent Test Cases\n",
    "\n",
    "### Test Case 1: Simple Retrieval\n",
    "\n",
    "**Query:** \"What is the status of the Summer Recipes post?\"\n",
    "- Agent must call `search_articles` (to get ID)\n",
    "- Expected trajectory: `[search_articles]`\n",
    "- Task completion: High (straightforward lookup)\n",
    "\n",
    "### Test Case 2: Complex Action with Logic\n",
    "\n",
    "**Query:** \"Find the Summer Recipes article and ensure it is published.\"\n",
    "- Agent should call `search_articles` (get ID)\n",
    "- Agent should call `get_article_details` (check current status)\n",
    "- Agent should **NOT** call `publish_article` (it's already published!)\n",
    "- Expected trajectory: `[search_articles, get_article_details]` â† No publish!\n",
    "- Task completion: High (task is complete without republishing)\n",
    "\n",
    "**Key Insight:** If the agent calls all three tools, it's being inefficient. Good evaluation catches this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f345d116",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_eval_dataset = [\n",
    "    # Case 1: Simple Retrieval\n",
    "    {\n",
    "        \"inputs\": {\"query\": \"What is the status of the Summer Recipes post?\"},\n",
    "        \"expectations\": {\n",
    "            \"expected_response\": \"It is currently published.\",\n",
    "            \"task_completion_threshold\": 1.0,\n",
    "            # The agent must call search, and the 'fact' it relies on is the tool output\n",
    "            \"expected_facts\": [EXPECTED_SEARCH_OUTPUT_102], \n",
    "            \"tool_call_trajectory\": [\"search_articles\"]\n",
    "        }\n",
    "    },\n",
    "    # Case 2: Complex Action (Search -> Check -> Publish)\n",
    "    {\n",
    "        \"inputs\": {\"query\": \"Find the Summer Recipes article and ensure it is published.\"},\n",
    "        \"expectations\": {\n",
    "            \"expected_response\": \"The article is already published.\",\n",
    "            # The agent should see the search result, AND the details showing it's published\n",
    "            \"expected_facts\": [EXPECTED_SEARCH_OUTPUT_102, EXPECTED_DETAILS_OUTPUT_102],\n",
    "            \"tool_call_trajectory\": [\"search_articles\", \"get_article_details\"] \n",
    "            # Note: It should NOT call publish_article because it sees it is already published\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3224197f",
   "metadata": {},
   "source": [
    "## 2.4 Custom Scorers for Agent Evaluation\n",
    "\n",
    "### Scorer 1: Task Completeness (Using DeepEval)\n",
    "\n",
    "**What It Measures:**\n",
    "- Did the agent accomplish the stated task?\n",
    "- Did it understand the requirements?\n",
    "- Did it reach a logical conclusion?\n",
    "\n",
    "**How It Works:**\n",
    "1. Extract tool calls from trace (What did the agent actually do?)\n",
    "2. Convert to DeepEval ToolCall objects\n",
    "3. Create test case with tools used\n",
    "4. DeepEval judges: \"Given these tools, was the task completed?\"\n",
    "\n",
    "**Example:**\n",
    "- Query: \"Find the Summer Recipes article and ensure it is published\"\n",
    "- Tools used: [search_articles, get_article_details]\n",
    "- Status quo: Article already published\n",
    "- Judge decision: âœ“ Task completed (republishing is redundant)\n",
    "\n",
    "### Scorer 2: Tool Trajectory Analysis\n",
    "\n",
    "**What It Measures:**\n",
    "- Did the agent follow the expected tool call sequence?\n",
    "- Is the trajectory logical given the inputs/outputs?\n",
    "\n",
    "**How It Works:**\n",
    "1. Extract actual tool calls from trace\n",
    "2. Compare to expected trajectory from test case\n",
    "3. Calculate match score (0.0 = no match, 1.0 = perfect match)\n",
    "4. Provide detailed feedback\n",
    "\n",
    "**Example:**\n",
    "- Expected: `[search_articles, get_article_details]`\n",
    "- Actual: `[search_articles, get_article_details, publish_article]`\n",
    "- Match score: 0.67 (2/3 tools match, but extra call reduces efficiency)\n",
    "\n",
    "---\n",
    "\n",
    "### Implementing the Scorers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d598745",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "@scorer(name=\"Task Completeness\")\n",
    "def task_completion_with_deepeval(trace: Trace, inputs: dict, outputs: str, expectations: dict) -> Feedback:\n",
    "    \"\"\"\n",
    "    Custom scorer that uses DeepEval's TaskCompletionMetric to evaluate task completion\n",
    "    based on trace analysis and tool calls\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Extract tool call information from the trace\n",
    "        tool_call_spans = trace.search_spans(span_type=SpanType.TOOL)\n",
    "\n",
    "        # Convert MLflow trace tool calls to DeepEval ToolCall format\n",
    "        tools_called = []\n",
    "        for span in tool_call_spans:\n",
    "            tool_call = ToolCall(\n",
    "                name=span.name,\n",
    "                description=span.attributes.get(\"description\", f\"Tool call for {span.name}\"),\n",
    "                input_parameters=span.inputs or {},\n",
    "                output=span.outputs or []\n",
    "            )\n",
    "            tools_called.append(tool_call)\n",
    "\n",
    "        # Extract the actual response text from the complex output structure\n",
    "        if isinstance(outputs, dict):\n",
    "            # Handle complex response structure\n",
    "            if 'response' in outputs and 'blocks' in outputs['response']:\n",
    "                actual_output = outputs['response']['blocks'][0]['text']\n",
    "            elif 'response' in outputs and isinstance(outputs['response'], str):\n",
    "                actual_output = outputs['response']\n",
    "            else:\n",
    "                actual_output = str(outputs)\n",
    "        elif isinstance(outputs, str):\n",
    "            actual_output = outputs\n",
    "        else:\n",
    "            actual_output = str(outputs)\n",
    "\n",
    "        # Create DeepEval test case\n",
    "        test_case = LLMTestCase(\n",
    "            input=inputs.get(\"query\", \"\"),\n",
    "            actual_output=actual_output,\n",
    "            tools_called=tools_called\n",
    "        )\n",
    "\n",
    "        # Initialize TaskCompletionMetric\n",
    "        threshold = expectations.get(\"task_completion_threshold\", 0.7)\n",
    "        metric = TaskCompletionMetric(\n",
    "            threshold=threshold,\n",
    "            model=\"gpt-4o\",  # Use consistent model\n",
    "            include_reason=True\n",
    "        )\n",
    "\n",
    "        # Run the metric evaluation\n",
    "        metric.measure(test_case)\n",
    "\n",
    "        # Extract results\n",
    "        score = metric.score\n",
    "        reason = metric.reason\n",
    "\n",
    "        return Feedback(\n",
    "            value=score,\n",
    "            rationale=f\"Task completion score: {score:.2f} (threshold: {threshold}). Tools used: {len(tools_called)}. {reason}\",\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        return Feedback(\n",
    "            value=0.0,\n",
    "            rationale=f\"Error evaluating task completion: {str(e)}\",\n",
    "            error=e\n",
    "        )\n",
    "\n",
    "\n",
    "@scorer(name=\"Tool Trajectory\")\n",
    "def tool_call_trajectory_analysis(trace: Trace, expectations: dict) -> Feedback:\n",
    "    \"\"\"\n",
    "    Analyze the tool call trajectory against expected sequence\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Search for tool call spans in the trace\n",
    "        tool_call_spans = trace.search_spans(span_type=SpanType.TOOL)\n",
    "\n",
    "        # Extract actual trajectory\n",
    "        actual_trajectory = [span.name for span in tool_call_spans]\n",
    "        expected_trajectory = expectations.get(\"tool_call_trajectory\", [])\n",
    "\n",
    "        # Calculate trajectory match\n",
    "        trajectory_match = actual_trajectory == expected_trajectory\n",
    "\n",
    "        # Calculate partial match score\n",
    "        if not expected_trajectory:\n",
    "            partial_score = 1.0 if actual_trajectory else 0.0\n",
    "        else:\n",
    "            # Calculate sequence similarity\n",
    "            min_len = min(len(actual_trajectory), len(expected_trajectory))\n",
    "            max_len = max(len(actual_trajectory), len(expected_trajectory))\n",
    "            if max_len == 0:\n",
    "                partial_score = 1.0\n",
    "            else:\n",
    "                matches = sum(1 for i in range(min_len)\n",
    "                             if i < len(actual_trajectory) and i < len(expected_trajectory)\n",
    "                             and actual_trajectory[i] == expected_trajectory[i])\n",
    "                partial_score = matches / max_len\n",
    "\n",
    "        return Feedback(\n",
    "            value=partial_score,\n",
    "            rationale=(\n",
    "                f\"Tool trajectory {'matches' if trajectory_match else 'differs from'} expectations. \"\n",
    "                f\"Expected: {expected_trajectory}. Actual: {actual_trajectory}. \"\n",
    "                f\"Match score: {partial_score:.2f}\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        return Feedback(\n",
    "            value=0.0,\n",
    "            rationale=f\"Error analyzing tool trajectory: {str(e)}\",\n",
    "            error=e\n",
    "        )\n",
    "    \n",
    "\n",
    "agent_scorers = [\n",
    "    task_completion_with_deepeval,\n",
    "    tool_call_trajectory_analysis,\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "086d0e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/24 21:15:07 INFO mlflow.genai.utils.data_validation: Testing model prediction with the first sample in the dataset. To disable this check, set the MLFLOW_GENAI_EVAL_SKIP_TRACE_VALIDATION environment variable to True.\n",
      "2025/11/24 21:15:07 WARNING mlflow.tracing.fluent: Failed to start span LangGraph: 'NonRecordingSpan' object has no attribute 'context'. For full traceback, set logging level to debug.\n",
      "Evaluating:   0%|          | 0/2 [Elapsed: 00:00, Remaining: ?] "
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/pedro.azevedo/dspt-mlflow/.venv/lib/python3.13/site-packages/rich/live.py:256: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/pedro.azevedo/dspt-mlflow/.venv/lib/python3.13/site-packages/rich/live.py:256: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/pedro.azevedo/dspt-mlflow/.venv/lib/python3.13/site-packages/rich/live.py:256: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/pedro.azevedo/dspt-mlflow/.venv/lib/python3.13/site-packages/rich/live.py:256: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/pedro.azevedo/dspt-mlflow/.venv/lib/python3.13/site-packages/rich/live.py:256: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/pedro.azevedo/dspt-mlflow/.venv/lib/python3.13/site-packages/rich/live.py:256: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/pedro.azevedo/dspt-mlflow/.venv/lib/python3.13/site-packages/rich/live.py:256: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/pedro.azevedo/dspt-mlflow/.venv/lib/python3.13/site-packages/rich/live.py:256: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/pedro.azevedo/dspt-mlflow/.venv/lib/python3.13/site-packages/rich/live.py:256: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/pedro.azevedo/dspt-mlflow/.venv/lib/python3.13/site-packages/rich/live.py:256: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [Elapsed: 00:03, Remaining: 00:03] "
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [Elapsed: 00:05, Remaining: 00:00] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ¨ Evaluation completed.\n",
      "\n",
      "Metrics and evaluation results are logged to the MLflow run:\n",
      "  Run name: \u001b[94mSimple Langgraph CMS Agent\u001b[0m\n",
      "  Run ID: \u001b[94ma77d1491668b4133a076296f1a2f1f8d\u001b[0m\n",
      "\n",
      "To view the detailed evaluation results with sample-wise scores,\n",
      "open the \u001b[93m\u001b[1mTraces\u001b[0m tab in the Run page in the MLflow UI.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "with mlflow.start_run(run_name=\"Simple Langgraph CMS Agent\"):\n",
    "\n",
    "    eval_results = mlflow.genai.evaluate(\n",
    "        data=agent_eval_dataset,\n",
    "        predict_fn=agent_predict_fn,\n",
    "        scorers=agent_scorers,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766a1906",
   "metadata": {},
   "source": [
    "## 2.5 Running the Agent Evaluation\n",
    "\n",
    "Let's evaluate our CMS agent using the same MLflow infrastructure as the RAG agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd922c7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Tool Trajectory/mean': np.float64(0.75),\n",
       " 'Task Completeness/mean': np.float64(0.85)}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Interpreting Agent Evaluation Results\n",
    "\n",
    "**Key Metrics to Check:**\n",
    "- **Task Completeness**: Is it â‰¥ 0.8? (Agent understood and completed tasks)\n",
    "- **Tool Trajectory Match**: Is it 1.0? (Agent followed expected call sequence)\n",
    "\n",
    "**What Low Scores Mean:**\n",
    "- Task Completeness < 0.7? Agent may not understand task requirements\n",
    "- Trajectory < 1.0? Agent may be using tools redundantly or out of order\n",
    "\n",
    "**Debugging Tips:**\n",
    "1. Check MLflow UI: Drill into specific test case\n",
    "2. Look at actual vs expected trajectory\n",
    "3. Check final agent outputâ€”did it answer correctly?\n",
    "4. Consider: Is your SOP clear enough? Might need better prompting\n",
    "\n",
    "---\n",
    "\n",
    "# Part 3: Synthesis & Key Takeaways\n",
    "\n",
    "## What We've Accomplished\n",
    "\n",
    "### RAG Evaluation\n",
    "âœ“ Built a retrieval pipeline with multi-hop test cases  \n",
    "âœ“ Configured scorers to verify both retrieval AND answer quality  \n",
    "âœ“ Used MLflow to track evaluation runs and results  \n",
    "\n",
    "### Agent Evaluation\n",
    "âœ“ Built an agentic system with multiple tools  \n",
    "âœ“ Designed test cases to verify efficiency AND correctness  \n",
    "âœ“ Created custom scorers to analyze tool trajectories  \n",
    "\n",
    "### EDD Workflow\n",
    "âœ“ Automated evaluation that runs with every code change  \n",
    "âœ“ Quantified metrics (not just \"it seems good\")  \n",
    "âœ“ Trace-based debugging (see exactly what the agent did)  \n",
    "\n",
    "---\n",
    "\n",
    "## Key Principles of Evaluation-Driven Development\n",
    "\n",
    "### 1. **Start with Clear Metrics**\n",
    "Define what \"good\" means before you build. (\"Correctness > 0.85\", \"No redundant tool calls\")\n",
    "\n",
    "### 2. **Design Realistic Test Cases**\n",
    "Single-hop is easy; test multi-hop scenarios that reveal real issues.\n",
    "\n",
    "### 3. **Use Automated Scoring**\n",
    "LLM-as-a-judge is imperfect but beats manual review. Scale with confidence.\n",
    "\n",
    "### 4. **Instrument for Debugging**\n",
    "Traces are gold. Capture everything. You can always ignore data, but you can't recover what wasn't logged.\n",
    "\n",
    "### 5. **Iterate Based on Metrics**\n",
    "When a metric is low, **drill into traces to understand why**. Then fix the root cause (prompt, tools, data).\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps: From Workshop to Production\n",
    "\n",
    "### For Your Own GenAI Application\n",
    "\n",
    "**1. Map Components to Your System**\n",
    "- What is your retrieval source? (Documents, APIs, databases)\n",
    "- What are your key tools?\n",
    "- What does \"success\" look like?\n",
    "\n",
    "**2. Design Your Test Suite**\n",
    "- Start with 5-10 critical test cases (not 100)\n",
    "- Include edge cases that previously failed\n",
    "- Update test suite as you discover new issues\n",
    "\n",
    "**3. Set Baseline Metrics**\n",
    "- Run evaluation today (establish \"current state\")\n",
    "- Define target metrics (e.g., Correctness > 0.85)\n",
    "- Evaluate before/after each change\n",
    "\n",
    "**4. Integrate into CI/CD**\n",
    "- Run evaluation on every commit\n",
    "- Fail builds if metrics drop below threshold\n",
    "- Use MLflow to track historical performance\n",
    "\n",
    "**5. Iterate in a Feedback Loop**\n",
    "```\n",
    "Evaluate â†’ Find Low Scores â†’ Understand Why (via Traces) \n",
    "  â†’ Improve (Prompt/Tools/Data) â†’ Evaluate Again\n",
    "```\n",
    "\n",
    "### Tools & Resources\n",
    "- **MLflow Docs**: https://mlflow.org/docs/latest/\n",
    "- **LangChain Agents**: https://python.langchain.com/docs/modules/agents/\n",
    "- **DeepEval Metrics**: https://docs.confident-ai.com/\n",
    "- **OpenAI Evals**: https://github.com/openai/evals (for inspiration)\n",
    "\n",
    "---\n",
    "\n",
    "## Questions? Further Experimentation?\n",
    "\n",
    "**Suggested Modifications to Try:**\n",
    "1. Add a new test case that you expect to failâ€”does it?\n",
    "2. Change the system promptâ€”how do scores change?\n",
    "3. Add a fourth tool to the CMS agentâ€”how does trajectory analysis adapt?\n",
    "4. Lower the similarity threshold (k=1 â†’ k=3)â€”does retrieval improve?\n",
    "\n",
    "**Conference Attendees**: Reach out on Slack/Discord with your questions. Happy to debug live! ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai-eval-demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
