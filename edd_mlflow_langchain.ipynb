{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1921ef3",
   "metadata": {},
   "source": [
    "# Eval Driven Development with MLflow & LangChain\n",
    "\n",
    "This notebook demonstrates how to perform **Evaluation Driven Development (EDD)** for GenAI applications using **MLflow 3.0+** and **LangChain**.\n",
    "\n",
    "We will cover two main scenarios:\n",
    "1.  **RAG Evaluation**: We will be using (`Correctness`, `Answer Relevancy`, `Context Relevancy`) to evaluate the RAG Agent.\n",
    "2.  **Agent Evaluation**: We will be using (`Task Completness`, `Tool Trajectory Analysis`) to evaluate the Agent traces and efficiency.\n",
    "\n",
    "### Prerequisites\n",
    "Ensure you have set your `OPENAI_API_KEY` in the environment or a `.env` file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619f3640",
   "metadata": {},
   "source": [
    "# Import Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "364e9d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !uv sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07a7f048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies if running in Colab or a fresh environment\n",
    "#%pip install -q \"mlflow>=2.14\" langgraph langchain langchain-openai langchain-community langchain-text-splitters faiss-cpu pandas openai python-dotenv bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbd8d61e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow version: 3.6.0\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "import os\n",
    "from langchain.tools import tool\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "from mlflow.genai.scorers import (\n",
    "    Correctness,\n",
    "    RelevanceToQuery,\n",
    "    Guidelines,\n",
    ")\n",
    "from mlflow.entities import Feedback, SpanType, Trace\n",
    "from mlflow.genai import scorer\n",
    "from deepeval.metrics import TaskCompletionMetric\n",
    "from deepeval.test_case import LLMTestCase, ToolCall\n",
    "import json\n",
    "from deepeval.metrics import ContextualRelevancyMetric\n",
    "\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "print(f\"MLflow version: {mlflow.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0f973e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pedro.azevedo/dspt-mlflow/.venv/lib/python3.13/site-packages/mlflow/tracking/_tracking_service/utils.py:140: FutureWarning: Filesystem tracking backend (e.g., './mlruns') is deprecated. Please switch to a database backend (e.g., 'sqlite:///mlflow.db'). For feedback, see: https://github.com/mlflow/mlflow/issues/18534\n",
      "  return FileStore(store_uri, store_uri)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///Users/pedro.azevedo/dspt-mlflow/mlruns/413835162552422093', creation_time=1763905836235, experiment_id='413835162552422093', last_update_time=1763905836235, lifecycle_stage='active', name='GenAI_Eval_Demo', tags={}>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load API Key\n",
    "load_dotenv()\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = input(\"Enter your OpenAI API Key: \")\n",
    "\n",
    "# Set a specific experiment for this notebook\n",
    "mlflow.set_experiment(\"GenAI_Eval_Demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26587c6",
   "metadata": {},
   "source": [
    "## Part 1: RAG Evaluation\n",
    "\n",
    "We will build a simple RAG chain that answers questions about software tools. We will then evaluate it using MLflow's **\"Trace Required\"** judges, which inspect the actual retrieved documents to ensure relevance and groundedness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac6fc73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Enable Autologging\n",
    "mlflow.langchain.autolog()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef18f079",
   "metadata": {},
   "source": [
    "### Helper Functions to Process Traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a104a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def extract_source_nodes(json_input):\n",
    "    \"\"\"\n",
    "    Parses a JSON string containing a message history and extracts source nodes\n",
    "    from tool artifacts.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        parsed_data = json.loads(json_input)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error parsing JSON: {e}\")\n",
    "        return []\n",
    "\n",
    "    # Handle the structure: {\"messages\": [...]}\n",
    "    messages = parsed_data.get(\"messages\", []) if isinstance(parsed_data, dict) else []\n",
    "    \n",
    "    source_nodes = []\n",
    "    \n",
    "    for message in messages:\n",
    "        # We are looking for messages where type is 'tool' and an 'artifact' list exists\n",
    "        if message.get(\"type\") == \"tool\" and \"artifact\" in message:\n",
    "            artifacts = message[\"artifact\"]\n",
    "            \n",
    "            # Ensure artifact is a list before extending our results\n",
    "            if isinstance(artifacts, list):\n",
    "                source_nodes.extend(artifacts)\n",
    "                \n",
    "    return source_nodes\n",
    "\n",
    "def extract_final_response(json_input):\n",
    "    \"\"\"\n",
    "    Parses a JSON string and extracts the content of the final AI response.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        parsed_data = json.loads(json_input)\n",
    "    except json.JSONDecodeError:\n",
    "        return None\n",
    "\n",
    "    messages = parsed_data.get(\"messages\", []) if isinstance(parsed_data, dict) else []\n",
    "    \n",
    "    # Iterate backwards to find the most recent AI message with content\n",
    "    for message in reversed(messages):\n",
    "        if message.get(\"type\") == \"ai\" and message.get(\"content\"):\n",
    "            return message[\"content\"]\n",
    "            \n",
    "    return None\n",
    "\n",
    "def _extract_deepeval_components(trace : Trace):\n",
    "    \"\"\"Extract input, output, and context from trace data\"\"\"\n",
    "    request = str(trace.data.request)\n",
    "    response = str(trace.data.response)\n",
    "\n",
    "    # extract source nodes if they exist\n",
    "    # Extract Source Nodes\n",
    "    outputs = extract_source_nodes(response)\n",
    "    retrieval_context = [node['page_content'] for node in outputs]\n",
    "    \n",
    "    \n",
    "    actual_output = extract_final_response(response)\n",
    "\n",
    "    return {\n",
    "        'input': request,\n",
    "        'actual_output': actual_output,\n",
    "        'retrieval_context': retrieval_context\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199af70d",
   "metadata": {},
   "source": [
    "## Part 2 RAG Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e523bf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define content as variables to ensure 100% match between VectorStore and Eval Dataset\n",
    "rag_content_phone = \"Orbit Phone X10 Specs: Runs OrbitOS 4.0, uses USB-C charging port, supports 5G, release date Jan 2024.\"\n",
    "rag_content_watch = \"Orbit Watch Pro Specs: Requires phone running OrbitOS 4.0 or higher to sync. Battery life 24h.\"\n",
    "rag_content_buds = \"Orbit Buds Lite Specs: Connects via Bluetooth 5.0. Compatible with any device supporting Bluetooth.\"\n",
    "rag_content_old_charger = \"Legacy Charger Adapter: This adapter converts Micro-USB to USB-C. Max output 5W.\"\n",
    "rag_content_new_charger = \"Orbit FastCharger: Native USB-C charger. Output 30W. Required for fast charging on X10.\"\n",
    "\n",
    "rag_docs = [\n",
    "    Document(page_content=rag_content_phone, metadata={\"id\": \"doc_1\"}),\n",
    "    Document(page_content=rag_content_watch, metadata={\"id\": \"doc_2\"}),\n",
    "    Document(page_content=rag_content_buds, metadata={\"id\": \"doc_3\"}),\n",
    "    Document(page_content=rag_content_old_charger, metadata={\"id\": \"doc_4\"}),\n",
    "    Document(page_content=rag_content_new_charger, metadata={\"id\": \"doc_5\"}),\n",
    "]\n",
    "\n",
    "# Create Vector Store & Retriever\n",
    "vectorstore = FAISS.from_documents(rag_docs, OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever(k=1)\n",
    "\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve_context(query: str):\n",
    "    \"\"\"Retrieve information to help answer a query.\"\"\"\n",
    "    retrieved_docs = vectorstore.similarity_search(query, k=2)\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        (f\"Source: {doc.metadata}\\nContent: {doc.page_content}\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    return serialized, retrieved_docs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86dbdb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_system_prompt = (\n",
    "    \"You are the Orbit Electronics Support Bot. \"\n",
    "    \"For every user question, you must retrieve specifications for ALL devices mentioned. \"\n",
    "    \"Synthesize the answer based ONLY on the retrieved text.\"\n",
    ")\n",
    "\n",
    "model = init_chat_model(\"gpt-4.1\")\n",
    "tools = [retrieve_context]\n",
    "\n",
    "\n",
    "rag_agent = create_agent(model, tools, system_prompt=rag_system_prompt)\n",
    "\n",
    "def qa_predict_rag_fn(query: str) -> str:\n",
    "    response = rag_agent.invoke({\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": query}],\n",
    "    })\n",
    "    answer = response['messages'][-1].content\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83999acf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Orbit Phone X10 uses a USB-C charging port. For fast charging, you need the Orbit FastCharger, which is a native USB-C charger with a 30W output.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_predict_rag_fn(\"What charger do I need for the Orbit Phone X10?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be1d158",
   "metadata": {},
   "source": [
    "## Define Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b954f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_eval_dataset = [\n",
    "    # Case 1: Multi-hop Compatibility\n",
    "    # Logic: User asks about Watch + Phone. \n",
    "    # Requirement: Must retrieve Phone Specs (OS version) AND Watch Specs (OS requirement).\n",
    "    {\n",
    "        \"inputs\": {\"query\": \"Can I use the Orbit Watch Pro with the Orbit Phone X10?\"},\n",
    "        \"expectations\": {\n",
    "            \"expected_response\": \"Yes. The Orbit Phone X10 runs OrbitOS 4.0, which matches the Orbit Watch Pro's requirement.\",\n",
    "            # VERBATIM MATCHES:\n",
    "            \"expected_facts\": [rag_content_phone, rag_content_watch],\n",
    "            \"retrieval_context\": [rag_content_phone, rag_content_watch] \n",
    "        }\n",
    "    },\n",
    "    # Case 2: Multi-hop Power/Charging\n",
    "    # Logic: User has Old Charger + New Phone. \n",
    "    # Requirement: Must retrieve Old Charger Specs (5W) AND New Charger Specs (Requirement for Fast Charge).\n",
    "    {\n",
    "        \"inputs\": {\"query\": \"Will the Legacy Charger Adapter allow me to fast charge my Orbit Phone X10?\"},\n",
    "        \"expectations\": {\n",
    "            \"expected_response\": \"No. The Legacy Adapter output is 5W, but the X10 requires 30W (Orbit FastCharger) to fast charge.\",\n",
    "            # VERBATIM MATCHES:\n",
    "            \"expected_facts\": [rag_content_old_charger, rag_content_new_charger],\n",
    "            \"retrieval_context\": [rag_content_old_charger, rag_content_new_charger]\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fae39fb",
   "metadata": {},
   "source": [
    "## Define Scorers (Evaluators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e23e3b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@scorer(name=\"Context Relevancy\")\n",
    "def contextual_relevancy(trace: Trace):\n",
    "    \"\"\"Evaluate RAG traces using DeepEval metrics\"\"\"\n",
    "    \n",
    "    data = _extract_deepeval_components(trace)\n",
    "\n",
    "    # Define DeepEval metrics   \n",
    "    \n",
    "\n",
    "    # Create test cases\n",
    "    test_case= LLMTestCase(\n",
    "            input=data['input'],\n",
    "            actual_output=data['actual_output'],\n",
    "            retrieval_context=data['retrieval_context']\n",
    "        )\n",
    "    \n",
    "    relevancy_metric = ContextualRelevancyMetric(threshold=0.7, model=\"gpt-4o-mini\",\n",
    "                                        include_reason=True)        \n",
    "        \n",
    "    # Run the metric evaluation\n",
    "    relevancy_metric.measure(test_case)\n",
    "\n",
    "    # Extract results\n",
    "    score = relevancy_metric.score\n",
    "    reason = relevancy_metric.reason\n",
    "\n",
    "    return Feedback(\n",
    "        value=score,\n",
    "        rationale=reason,\n",
    "    )\n",
    "\n",
    "# Business-specific guidelines\n",
    "business_guidelines = Guidelines(\n",
    "    name=\"toxicity_clear\",\n",
    "    guidelines=\"\"\"\n",
    "    The response should avoid toxic language and adhere to community guidelines.\n",
    "    \"\"\",\n",
    ")\n",
    "\n",
    "# Configure RAG-specific scorers\n",
    "rag_scorers = [\n",
    "    Correctness(\n",
    "       # model=\"litellm_proxy:/amazon.nova-micro-v1:0\",\n",
    "    ),\n",
    "    RelevanceToQuery(\n",
    "        name=\"AnswerRelevance\"\n",
    "        #model=\"litellm_proxy:/amazon.nova-micro-v1:0\",\n",
    "    ),\n",
    "    contextual_relevancy,\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67389f08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/24 21:14:12 INFO mlflow.models.evaluation.utils.trace: Auto tracing is temporarily enabled during the model evaluation for computing some metrics and debugging. To disable tracing, call `mlflow.autolog(disable=True)`.\n",
      "/Users/pedro.azevedo/dspt-mlflow/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025/11/24 21:14:13 INFO mlflow.genai.utils.data_validation: Testing model prediction with the first sample in the dataset. To disable this check, set the MLFLOW_GENAI_EVAL_SKIP_TRACE_VALIDATION environment variable to True.\n",
      "2025/11/24 21:14:13 WARNING mlflow.tracing.fluent: Failed to start span LangGraph: 'NonRecordingSpan' object has no attribute 'context'. For full traceback, set logging level to debug.\n",
      "Evaluating:   0%|          | 0/2 [Elapsed: 00:00, Remaining: ?] "
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/pedro.azevedo/dspt-mlflow/.venv/lib/python3.13/site-packages/rich/live.py:256: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/pedro.azevedo/dspt-mlflow/.venv/lib/python3.13/site-packages/rich/live.py:256: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/pedro.azevedo/dspt-mlflow/.venv/lib/python3.13/site-packages/rich/live.py:256: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/pedro.azevedo/dspt-mlflow/.venv/lib/python3.13/site-packages/rich/live.py:256: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/pedro.azevedo/dspt-mlflow/.venv/lib/python3.13/site-packages/rich/live.py:256: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/pedro.azevedo/dspt-mlflow/.venv/lib/python3.13/site-packages/rich/live.py:256: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/pedro.azevedo/dspt-mlflow/.venv/lib/python3.13/site-packages/rich/live.py:256: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/pedro.azevedo/dspt-mlflow/.venv/lib/python3.13/site-packages/rich/live.py:256: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/pedro.azevedo/dspt-mlflow/.venv/lib/python3.13/site-packages/rich/live.py:256: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/pedro.azevedo/dspt-mlflow/.venv/lib/python3.13/site-packages/rich/live.py:256: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  50%|█████     | 1/2 [Elapsed: 00:07, Remaining: 00:07] "
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 2/2 [Elapsed: 00:09, Remaining: 00:00] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✨ Evaluation completed.\n",
      "\n",
      "Metrics and evaluation results are logged to the MLflow run:\n",
      "  Run name: \u001b[94mSimple Langgraph Agent\u001b[0m\n",
      "  Run ID: \u001b[94m0cb62d5b82ef485cb37048232864abc9\u001b[0m\n",
      "\n",
      "To view the detailed evaluation results with sample-wise scores,\n",
      "open the \u001b[93m\u001b[1mTraces\u001b[0m tab in the Run page in the MLflow UI.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "with mlflow.start_run(run_name=\"Simple Langgraph Agent\"):\n",
    "\n",
    "    eval_results = mlflow.genai.evaluate(\n",
    "        data=rag_eval_dataset,\n",
    "        predict_fn=qa_predict_rag_fn,\n",
    "        scorers=rag_scorers,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bde0a6",
   "metadata": {},
   "source": [
    "eval_results.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "381c4d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mock Database\n",
    "cms_db = {\n",
    "    \"101\": {\"title\": \"AI Trends 2024\", \"status\": \"draft\", \"tags\": [\"tech\"]},\n",
    "    \"102\": {\"title\": \"Summer Recipes\", \"status\": \"published\", \"tags\": [\"food\"]},\n",
    "}\n",
    "\n",
    "# We define expected output strings for our test cases to verify against\n",
    "EXPECTED_SEARCH_OUTPUT_102 = str([{\"id\": \"102\", \"title\": \"Summer Recipes\", \"status\": \"published\"}])\n",
    "EXPECTED_DETAILS_OUTPUT_102 = str({\"title\": \"Summer Recipes\", \"status\": \"published\", \"tags\": [\"food\"]})\n",
    "\n",
    "\n",
    "# Tools \n",
    "@tool\n",
    "def search_articles(query: str):\n",
    "    \"\"\"Searches for articles by title. Returns JSON string of matches.\"\"\"\n",
    "    # Simple logic to mimic a search engine\n",
    "    results = [{\"id\": k, \"title\": v[\"title\"], \"status\": v[\"status\"]} \n",
    "               for k, v in cms_db.items() if query.lower() in v[\"title\"].lower()]\n",
    "    return str(results)\n",
    "\n",
    "@tool\n",
    "def get_article_details(article_id: str):\n",
    "    \"\"\"Gets full details for an ID.\"\"\"\n",
    "    return str(cms_db.get(article_id, \"Article not found\"))\n",
    "\n",
    "@tool\n",
    "def publish_article(article_id: str):\n",
    "    \"\"\"Publishes an article.\"\"\"\n",
    "    if article_id in cms_db:\n",
    "        cms_db[article_id][\"status\"] = \"published\"\n",
    "        return f\"Success: Article {article_id} published.\"\n",
    "    return \"Error: ID not found.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d2b53dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Tools + LLM\n",
    "cms_tools = [search_articles, get_article_details, publish_article]\n",
    "chat_model = init_chat_model(\"gpt-4.1\")\n",
    "\n",
    "# Setup Prompt\n",
    "agent_system_prompt = (\n",
    "    \"You are a CMS Manager. \"\n",
    "    \"SOP: Always SEARCH for an article ID first. Never guess IDs. \"\n",
    "    \"Before publishing, GET DETAILS to confirm the current status.\"\n",
    ")\n",
    "\n",
    "# Setup Agent\n",
    "cms_agent = create_agent(model=chat_model, tools=cms_tools, system_prompt=agent_system_prompt)\n",
    "\n",
    "# Prediction Function\n",
    "def agent_predict_fn(query: str) -> str:\n",
    "    response = cms_agent.invoke({\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": query}],\n",
    "    })\n",
    "    answer = response['messages'][-1].content\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd5ba0e",
   "metadata": {},
   "source": [
    "## Setup Agent Eval Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f345d116",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_eval_dataset = [\n",
    "    # Case 1: Simple Retrieval\n",
    "    {\n",
    "        \"inputs\": {\"query\": \"What is the status of the Summer Recipes post?\"},\n",
    "        \"expectations\": {\n",
    "            \"expected_response\": \"It is currently published.\",\n",
    "            \"task_completion_threshold\": 1.0,\n",
    "            # The agent must call search, and the 'fact' it relies on is the tool output\n",
    "            \"expected_facts\": [EXPECTED_SEARCH_OUTPUT_102], \n",
    "            \"tool_call_trajectory\": [\"search_articles\"]\n",
    "        }\n",
    "    },\n",
    "    # Case 2: Complex Action (Search -> Check -> Publish)\n",
    "    {\n",
    "        \"inputs\": {\"query\": \"Find the Summer Recipes article and ensure it is published.\"},\n",
    "        \"expectations\": {\n",
    "            \"expected_response\": \"The article is already published.\",\n",
    "            # The agent should see the search result, AND the details showing it's published\n",
    "            \"expected_facts\": [EXPECTED_SEARCH_OUTPUT_102, EXPECTED_DETAILS_OUTPUT_102],\n",
    "            \"tool_call_trajectory\": [\"search_articles\", \"get_article_details\"] \n",
    "            # Note: It should NOT call publish_article because it sees it is already published\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3224197f",
   "metadata": {},
   "source": [
    "## Setup Scorers and Evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d598745",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "@scorer(name=\"Task Completeness\")\n",
    "def task_completion_with_deepeval(trace: Trace, inputs: dict, outputs: str, expectations: dict) -> Feedback:\n",
    "    \"\"\"\n",
    "    Custom scorer that uses DeepEval's TaskCompletionMetric to evaluate task completion\n",
    "    based on trace analysis and tool calls\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Extract tool call information from the trace\n",
    "        tool_call_spans = trace.search_spans(span_type=SpanType.TOOL)\n",
    "\n",
    "        # Convert MLflow trace tool calls to DeepEval ToolCall format\n",
    "        tools_called = []\n",
    "        for span in tool_call_spans:\n",
    "            tool_call = ToolCall(\n",
    "                name=span.name,\n",
    "                description=span.attributes.get(\"description\", f\"Tool call for {span.name}\"),\n",
    "                input_parameters=span.inputs or {},\n",
    "                output=span.outputs or []\n",
    "            )\n",
    "            tools_called.append(tool_call)\n",
    "\n",
    "        # Extract the actual response text from the complex output structure\n",
    "        if isinstance(outputs, dict):\n",
    "            # Handle complex response structure\n",
    "            if 'response' in outputs and 'blocks' in outputs['response']:\n",
    "                actual_output = outputs['response']['blocks'][0]['text']\n",
    "            elif 'response' in outputs and isinstance(outputs['response'], str):\n",
    "                actual_output = outputs['response']\n",
    "            else:\n",
    "                actual_output = str(outputs)\n",
    "        elif isinstance(outputs, str):\n",
    "            actual_output = outputs\n",
    "        else:\n",
    "            actual_output = str(outputs)\n",
    "\n",
    "        # Create DeepEval test case\n",
    "        test_case = LLMTestCase(\n",
    "            input=inputs.get(\"query\", \"\"),\n",
    "            actual_output=actual_output,\n",
    "            tools_called=tools_called\n",
    "        )\n",
    "\n",
    "        # Initialize TaskCompletionMetric\n",
    "        threshold = expectations.get(\"task_completion_threshold\", 0.7)\n",
    "        metric = TaskCompletionMetric(\n",
    "            threshold=threshold,\n",
    "            model=\"gpt-4o\",  # Use consistent model\n",
    "            include_reason=True\n",
    "        )\n",
    "\n",
    "        # Run the metric evaluation\n",
    "        metric.measure(test_case)\n",
    "\n",
    "        # Extract results\n",
    "        score = metric.score\n",
    "        reason = metric.reason\n",
    "\n",
    "        return Feedback(\n",
    "            value=score,\n",
    "            rationale=f\"Task completion score: {score:.2f} (threshold: {threshold}). Tools used: {len(tools_called)}. {reason}\",\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        return Feedback(\n",
    "            value=0.0,\n",
    "            rationale=f\"Error evaluating task completion: {str(e)}\",\n",
    "            error=e\n",
    "        )\n",
    "\n",
    "\n",
    "@scorer(name=\"Tool Trajectory\")\n",
    "def tool_call_trajectory_analysis(trace: Trace, expectations: dict) -> Feedback:\n",
    "    \"\"\"\n",
    "    Analyze the tool call trajectory against expected sequence\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Search for tool call spans in the trace\n",
    "        tool_call_spans = trace.search_spans(span_type=SpanType.TOOL)\n",
    "\n",
    "        # Extract actual trajectory\n",
    "        actual_trajectory = [span.name for span in tool_call_spans]\n",
    "        expected_trajectory = expectations.get(\"tool_call_trajectory\", [])\n",
    "\n",
    "        # Calculate trajectory match\n",
    "        trajectory_match = actual_trajectory == expected_trajectory\n",
    "\n",
    "        # Calculate partial match score\n",
    "        if not expected_trajectory:\n",
    "            partial_score = 1.0 if actual_trajectory else 0.0\n",
    "        else:\n",
    "            # Calculate sequence similarity\n",
    "            min_len = min(len(actual_trajectory), len(expected_trajectory))\n",
    "            max_len = max(len(actual_trajectory), len(expected_trajectory))\n",
    "            if max_len == 0:\n",
    "                partial_score = 1.0\n",
    "            else:\n",
    "                matches = sum(1 for i in range(min_len)\n",
    "                             if i < len(actual_trajectory) and i < len(expected_trajectory)\n",
    "                             and actual_trajectory[i] == expected_trajectory[i])\n",
    "                partial_score = matches / max_len\n",
    "\n",
    "        return Feedback(\n",
    "            value=partial_score,\n",
    "            rationale=(\n",
    "                f\"Tool trajectory {'matches' if trajectory_match else 'differs from'} expectations. \"\n",
    "                f\"Expected: {expected_trajectory}. Actual: {actual_trajectory}. \"\n",
    "                f\"Match score: {partial_score:.2f}\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        return Feedback(\n",
    "            value=0.0,\n",
    "            rationale=f\"Error analyzing tool trajectory: {str(e)}\",\n",
    "            error=e\n",
    "        )\n",
    "    \n",
    "\n",
    "agent_scorers = [\n",
    "    task_completion_with_deepeval,\n",
    "    tool_call_trajectory_analysis,\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "086d0e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/24 21:15:07 INFO mlflow.genai.utils.data_validation: Testing model prediction with the first sample in the dataset. To disable this check, set the MLFLOW_GENAI_EVAL_SKIP_TRACE_VALIDATION environment variable to True.\n",
      "2025/11/24 21:15:07 WARNING mlflow.tracing.fluent: Failed to start span LangGraph: 'NonRecordingSpan' object has no attribute 'context'. For full traceback, set logging level to debug.\n",
      "Evaluating:   0%|          | 0/2 [Elapsed: 00:00, Remaining: ?] "
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/pedro.azevedo/dspt-mlflow/.venv/lib/python3.13/site-packages/rich/live.py:256: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/pedro.azevedo/dspt-mlflow/.venv/lib/python3.13/site-packages/rich/live.py:256: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/pedro.azevedo/dspt-mlflow/.venv/lib/python3.13/site-packages/rich/live.py:256: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/pedro.azevedo/dspt-mlflow/.venv/lib/python3.13/site-packages/rich/live.py:256: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/pedro.azevedo/dspt-mlflow/.venv/lib/python3.13/site-packages/rich/live.py:256: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/pedro.azevedo/dspt-mlflow/.venv/lib/python3.13/site-packages/rich/live.py:256: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/pedro.azevedo/dspt-mlflow/.venv/lib/python3.13/site-packages/rich/live.py:256: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/pedro.azevedo/dspt-mlflow/.venv/lib/python3.13/site-packages/rich/live.py:256: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/pedro.azevedo/dspt-mlflow/.venv/lib/python3.13/site-packages/rich/live.py:256: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/pedro.azevedo/dspt-mlflow/.venv/lib/python3.13/site-packages/rich/live.py:256: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  50%|█████     | 1/2 [Elapsed: 00:03, Remaining: 00:03] "
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 2/2 [Elapsed: 00:05, Remaining: 00:00] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✨ Evaluation completed.\n",
      "\n",
      "Metrics and evaluation results are logged to the MLflow run:\n",
      "  Run name: \u001b[94mSimple Langgraph CMS Agent\u001b[0m\n",
      "  Run ID: \u001b[94ma77d1491668b4133a076296f1a2f1f8d\u001b[0m\n",
      "\n",
      "To view the detailed evaluation results with sample-wise scores,\n",
      "open the \u001b[93m\u001b[1mTraces\u001b[0m tab in the Run page in the MLflow UI.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "with mlflow.start_run(run_name=\"Simple Langgraph CMS Agent\"):\n",
    "\n",
    "    eval_results = mlflow.genai.evaluate(\n",
    "        data=agent_eval_dataset,\n",
    "        predict_fn=agent_predict_fn,\n",
    "        scorers=agent_scorers,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd922c7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Tool Trajectory/mean': np.float64(0.75),\n",
       " 'Task Completeness/mean': np.float64(0.85)}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_results.metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai-eval-demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
